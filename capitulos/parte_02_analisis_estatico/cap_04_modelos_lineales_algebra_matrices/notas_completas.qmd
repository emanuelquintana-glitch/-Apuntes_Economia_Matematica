---
title: "Capítulo 4: Modelos Lineales y Álgebra de Matrices"
subtitle: "Fundamentos de Economía Matemática"
author: 
  - name: "Emanuel Quintana Silva"
    affiliation: "Universidad Pedagógica y Tecnológica de Colombia (UPTC)"
    email: "emanuel.quintana@uptc.edu.co"
    orcid: "0009-0006-8419-2805"
date: "Diciembre 2025"
date-format: "MMMM YYYY"

format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 4
    toc-location: left
    toc-title: "Contenido"
    number-sections: true
    number-depth: 4
    code-fold: show
    code-summary: "Mostrar código"
    code-tools: true
    code-copy: true
    code-overflow: wrap
    html-math-method: mathjax
    
  pdf:
    documentclass: book
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
    number-sections: true
    toc: true
    toc-depth: 3
    colorlinks: true

execute:
  echo: true
  warning: false
  message: false
  cache: true

jupyter: python3
lang: es
---

```{python}
#| echo: false
#| output: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sympy as sp
from sympy import Matrix, symbols, latex, simplify, solve, det, eye
from IPython.display import display, Markdown, Latex
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3

sp.init_printing(use_latex='mathjax')
```

::: {.callout-note icon=false}
## Información del Capítulo

**Basado en:** Chiang, A. C. & Wainwright, K. (2005). *Fundamental Methods of Mathematical Economics*. McGraw-Hill.

**Objetivo:** Introducir el álgebra de matrices como herramienta fundamental para el análisis de sistemas lineales en economía matemática, con aplicaciones a modelos de equilibrio de mercado, ingreso nacional y análisis insumo-producto.
:::

# Introducción al Álgebra Matricial

## El Desafío Dimensional en Economía

En economía matemática, el análisis de modelos con múltiples variables requiere metodologías que vayan más allá de las técnicas escalares tradicionales. El **álgebra de matrices** emerge como la herramienta fundamental para abordar el análisis estático o de equilibrio cuando el número de variables es grande.

::: {.callout-important}
## Necesidad del Álgebra Matricial

Cuando un modelo de mercado aumenta de uno a dos o más artículos, las fórmulas de eliminación de variables se vuelven rápidamente inmanejables. El álgebra de matrices proporciona una solución sistemática y elegante a este problema de complejidad.
:::

### Tres Ventajas Fundamentales

El álgebra de matrices ofrece ventajas cruciales para el análisis económico:

1. **Compacidad Notacional**: Proporciona una forma compacta de escribir sistemas de ecuaciones, incluso muy grandes
2. **Teoría de Existencia**: Permite probar la existencia de soluciones mediante la evaluación de determinantes
3. **Método Sistemático**: Suministra un procedimiento algorítmico para encontrar soluciones cuando existen

### Linealidad y Transformaciones

El álgebra de matrices se aplica específicamente a **sistemas de ecuaciones lineales**. Sin embargo, esta restricción no es limitante: modelos no lineales pueden transformarse mediante cambios de variables en relaciones lineales equivalentes.

**Ejemplo de Transformación:**
$$y = ax^b \quad \xrightarrow{\log} \quad \log y = \log a + b\log x$$

```{python}
#| label: fig-transformacion-log
#| fig-cap: "Transformación logarítmica de función no lineal a lineal"

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

x = np.linspace(0.1, 10, 100)
a, b = 2, 1.5
y = a * x**b

# Gráfica original (no lineal)
ax1.plot(x, y, 'b-', linewidth=2, label=f'$y = {a}x^{{{b}}}$')
ax1.set_xlabel('x', fontsize=12)
ax1.set_ylabel('y', fontsize=12)
ax1.set_title('Función Original (No Lineal)', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Gráfica transformada (lineal)
log_x = np.log(x)
log_y = np.log(y)
ax2.plot(log_x, log_y, 'r-', linewidth=2, label=f'$\\log y = \\log {a} + {b}\\log x$')
ax2.set_xlabel('log(x)', fontsize=12)
ax2.set_ylabel('log(y)', fontsize=12)
ax2.set_title('Función Transformada (Lineal)', fontsize=14, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

# Matrices como Arreglos Rectangulares

## Definición Formal

Una **matriz** se define como un **arreglo rectangular** de números, parámetros o variables. Los elementos individuales se identifican mediante subíndices dobles.

::: {.callout-tip}
## Definición: Matriz

Una matriz $\mathbf{A}$ de dimensión $m \times n$ es un arreglo rectangular de $m$ filas y $n$ columnas:

$$\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}$$

Donde $a_{ij}$ denota el elemento en la fila $i$ y columna $j$.
:::

```{python}
# Ejemplo de matriz
A = np.array([
    [6, 3, 1],
    [1, 4, -2],
    [4, -1, 5]
])

print("Matriz A (3×3):")
print(A)
print(f"\nDimensión: {A.shape[0]}×{A.shape[1]}")
print(f"Elemento a₂₃ = {A[1, 2]}")
```

## Representación de Sistemas Lineales

Un sistema de $m$ ecuaciones lineales en $n$ variables se puede escribir de manera compacta mediante la **notación matricial $\mathbf{Ax = d}$**.

Para el sistema general:
$$\begin{cases}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = d_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = d_2 \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = d_m
\end{cases}$$

La representación matricial es:
$$\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
=
\begin{bmatrix} d_1 \\ d_2 \\ \vdots \\ d_m \end{bmatrix}$$

Donde:
- $\mathbf{A}$: Matriz de coeficientes $(m \times n)$
- $\mathbf{x}$: Vector de variables $(n \times 1)$
- $\mathbf{d}$: Vector de términos constantes $(m \times 1)$

```{python}
# Sistema matricial ejemplo
A_sym = Matrix([[6, 3, 1], [1, 4, -2], [4, -1, 5]])
x_sym = Matrix([symbols('x_1'), symbols('x_2'), symbols('x_3')])
d_sym = Matrix([symbols('d_1'), symbols('d_2'), symbols('d_3')])

print("Sistema: Ax = d")
print("\nMatriz A:")
display(A_sym)
print("\nVector x:")
display(x_sym)
print("\nVector d:")
display(d_sym)
```

# Vectores como Matrices Especiales

## Tipos de Vectores

Un **vector** es una matriz que posee una única columna o un único renglón.

::: {.callout-tip}
## Clasificación Vectorial

**Vector Columna** ($m \times 1$):
$$\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{bmatrix}$$

**Vector Renglón** ($1 \times n$):
$$\mathbf{x}' = \begin{bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}$$
:::

```{python}
# Vectores columna y renglón
v_columna = np.array([[3], [2], [1]])
v_renglon = np.array([[1, 4, 5]])

print("Vector columna (3×1):")
print(v_columna)
print(f"Dimensión: {v_columna.shape}")

print("\nVector renglón (1×3):")
print(v_renglon)
print(f"Dimensión: {v_renglon.shape}")
```

## Interpretación Geométrica

Todo vector con $n$ componentes puede interpretarse como una **$n$-tupla ordenada**, equivalente a un punto en el espacio euclidiano $\mathbb{R}^n$.

```{python}
#| label: fig-vectores-geometrico
#| fig-cap: "Interpretación geométrica de vectores en ℝ²"

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111)

# Vectores
v1 = np.array([3, 2])
v2 = np.array([1, 4])
v_suma = v1 + v2

# Dibujar vectores
ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, 
          color='red', width=0.006, label=r'$\mathbf{v}_1 = (3, 2)$')
ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, 
          color='blue', width=0.006, label=r'$\mathbf{v}_2 = (1, 4)$')
ax.quiver(0, 0, v_suma[0], v_suma[1], angles='xy', scale_units='xy', scale=1, 
          color='green', width=0.008, label=r'$\mathbf{v}_1 + \mathbf{v}_2 = (4, 6)$')

# Configuración
ax.set_xlim(-0.5, 5)
ax.set_ylim(-0.5, 7)
ax.set_xlabel('$x_1$', fontsize=12)
ax.set_ylabel('$x_2$', fontsize=12)
ax.set_title('Vectores en $\mathbb{R}^2$', fontsize=14, fontweight='bold')
ax.axhline(y=0, color='k', linewidth=0.5)
ax.axvline(x=0, color='k', linewidth=0.5)
ax.grid(True, alpha=0.3)
ax.legend(fontsize=11)
ax.set_aspect('equal')

plt.tight_layout()
plt.show()
```

## Producto Escalar (Norma)

El **producto escalar** o **producto punto** de un vector consigo mismo define su norma (longitud euclidiana):

$$\|\mathbf{x}\| = \sqrt{\mathbf{x}'\mathbf{x}} = \sqrt{\sum_{i=1}^{n} x_i^2}$$

```{python}
# Norma de un vector
v = np.array([[3], [4]])
norma = np.linalg.norm(v)
producto_escalar = np.dot(v.T, v)[0,0]

print(f"Vector v: {v.T}")
print(f"Producto escalar v'v = {producto_escalar}")
print(f"Norma ||v|| = √{producto_escalar} = {norma:.4f}")
```

# Operaciones Fundamentales con Matrices

## Igualdad de Matrices

::: {.callout-tip}
## Definición: Igualdad Matricial

Dos matrices $\mathbf{A} = [a_{ij}]$ y $\mathbf{B} = [b_{ij}]$ son iguales si y solo si:

1. Poseen la misma dimensión: $m \times n$
2. Todos los elementos correspondientes son idénticos: $a_{ij} = b_{ij}$ para todo $i, j$
:::

## Suma y Resta de Matrices

**Condición de Conformabilidad:** Las matrices deben tener la **misma dimensión**.

$$\mathbf{C} = \mathbf{A} + \mathbf{B} \implies c_{ij} = a_{ij} + b_{ij}$$

**Propiedades:**
- **Conmutativa**: $\mathbf{A + B = B + A}$
- **Asociativa**: $\mathbf{(A + B) + C = A + (B + C)}$

```{python}
# Suma de matrices
A = np.array([[4, 9], [2, 1]])
B = np.array([[2, 0], [0, 7]])
C = A + B

print("Matriz A:")
print(A)
print("\nMatriz B:")
print(B)
print("\nMatriz C = A + B:")
print(C)
print("\n¿A + B = B + A?", np.array_equal(A + B, B + A))
```

## Multiplicación Escalar

Multiplicar una matriz por un escalar $\alpha$ implica multiplicar cada elemento:

$$\alpha\mathbf{A} = [\alpha a_{ij}]$$

**Propiedades:**
- $\alpha(\mathbf{A + B}) = \alpha\mathbf{A} + \alpha\mathbf{B}$ (Distributiva)
- $(\alpha + \beta)\mathbf{A} = \alpha\mathbf{A} + \beta\mathbf{A}$ (Distributiva)

```{python}
# Multiplicación escalar
A = np.array([[3, -1], [0, 5]])
alpha = 7
resultado = alpha * A

print(f"Matriz A:")
print(A)
print(f"\nEscalar α = {alpha}")
print(f"\nResultado {alpha}A:")
print(resultado)
```

# Multiplicación de Matrices

## Condición de Conformabilidad

::: {.callout-important}
## Regla Dimensional

El producto $\mathbf{AB}$ está definido si y solo si:

$$\mathbf{A}_{m \times n} \times \mathbf{B}_{n \times q} = \mathbf{C}_{m \times q}$$

El número de **columnas** de $\mathbf{A}$ debe ser igual al número de **filas** de $\mathbf{B}$.
:::

## Definición del Producto

Cada elemento $c_{ij}$ de la matriz producto se define como el **producto interior** de la fila $i$ de $\mathbf{A}$ con la columna $j$ de $\mathbf{B}$:

$$c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj}$$

```{python}
# Multiplicación de matrices
A = np.array([[2, 3], [1, 4]])
B = np.array([[5, 1], [2, 3]])
C = np.dot(A, B)

print("Matriz A (2×2):")
print(A)
print("\nMatriz B (2×2):")
print(B)
print("\nMatriz C = AB:")
print(C)

# Verificación elemento por elemento
print(f"\nc₁₁ = {A[0,0]}·{B[0,0]} + {A[0,1]}·{B[1,0]} = {A[0,0]*B[0,0] + A[0,1]*B[1,0]}")
print(f"c₁₂ = {A[0,0]}·{B[0,1]} + {A[0,1]}·{B[1,1]} = {A[0,0]*B[0,1] + A[0,1]*B[1,1]}")
```

## Propiedades Algebraicas

::: {.callout-warning}
## No Conmutatividad

En general: $\mathbf{AB} \neq \mathbf{BA}$

El orden de los factores **sí altera** el producto. Esta es la diferencia más importante respecto al álgebra escalar.
:::

```{python}
# Demostración de no conmutatividad
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

AB = np.dot(A, B)
BA = np.dot(B, A)

print("AB:")
print(AB)
print("\nBA:")
print(BA)
print(f"\n¿AB = BA? {np.array_equal(AB, BA)}")
```

::: {.callout-tip}
## Propiedades Importantes

1. **Asociativa**: $(\mathbf{AB})\mathbf{C} = \mathbf{A}(\mathbf{BC}) = \mathbf{ABC}$
2. **Distributiva**: $\mathbf{A(B + C) = AB + AC}$ y $\mathbf{(A + B)C = AC + BC}$
3. **Transpuesta del producto**: $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$ (orden inverso)
:::

# Multiplicación de Vectores

## Vector Columna × Vector Renglón (Producto Exterior)

**Producto Exterior** ($\mathbf{uv}'$): Genera una **matriz**

$$\mathbf{u}_{m \times 1} \times \mathbf{v}'_{1 \times n} = \mathbf{M}_{m \times n}$$

```{python}
# Producto exterior
u = np.array([[3], [2]])
v_t = np.array([[1, 4, 5]])

M = np.dot(u, v_t)

print("Vector u (columna):")
print(u)
print("\nVector v' (renglón):")
print(v_t)
print("\nProducto exterior uv':")
print(M)
```

## Vector Renglón × Vector Columna (Producto Interior)

**Producto Interior** ($\mathbf{u}'\mathbf{v}$): Genera un **escalar**

$$\mathbf{u}'_{1 \times n} \times \mathbf{v}_{n \times 1} = s_{1 \times 1}$$

$$\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_iv_i$$

```{python}
# Producto interior (producto punto)
u_t = np.array([[3, 4]])
v = np.array([[9], [7]])

s = np.dot(u_t, v)

print("Vector u' (renglón):")
print(u_t)
print("\nVector v (columna):")
print(v)
print("\nProducto interior u'v:")
print(s)
print(f"\nCálculo: 3(9) + 4(7) = {3*9 + 4*7}")
```

## Aplicación Económica: Costo Total

```{python}
# Ejemplo: Costo total de compra
cantidades = np.array([[10, 5, 3]])  # Q' (renglón)
precios = np.array([[50], [100], [200]])  # P (columna)

costo_total = np.dot(cantidades, precios)[0, 0]

print("Cantidades Q' (unidades):")
print(cantidades)
print("\nPrecios P ($/unidad):")
print(precios)
print(f"\nCosto Total = Q'·P = ${costo_total:,.0f}")
```

# Matrices Especiales

## Matrices Cuadradas

Una matriz es **cuadrada** cuando tiene el mismo número de filas que de columnas ($m = n$).

**Propiedades especiales:**
- Solo las matrices cuadradas pueden tener inversa
- El determinante solo está definido para matrices cuadradas
- Las potencias de matrices solo están definidas para matrices cuadradas

### Diagonal Principal

Los elementos $a_{ii}$ donde $i = j$ forman la **diagonal principal**:

$$\text{diag}(\mathbf{A}) = \{a_{11}, a_{22}, \ldots, a_{nn}\}$$

### Traza

La **traza** de una matriz cuadrada es la suma de los elementos de su diagonal principal:

$$\text{tr}(\mathbf{A}) = \sum_{i=1}^{n} a_{ii}$$

```{python}
# Matriz cuadrada y sus propiedades
A = np.array([
    [5, 2, 1],
    [3, 7, 4],
    [2, 1, 9]
])

diagonal = np.diag(A)
traza = np.trace(A)

print("Matriz cuadrada A (3×3):")
print(A)
print(f"\nDiagonal principal: {diagonal}")
print(f"Traza tr(A) = {traza}")
```

## Matriz Diagonal

Una matriz cuadrada donde todos los elementos fuera de la diagonal principal son cero:

$$\mathbf{D} = \begin{bmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_n
\end{bmatrix}$$

**Propiedades:**
- Producto de matrices diagonales es diagonal
- La potencia n-ésima: $\mathbf{D}^n = \text{diag}(d_1^n, d_2^n, \ldots, d_n^n)$

```{python}
# Matriz diagonal
D = np.diag([3, -1, 5])

print("Matriz diagonal D:")
print(D)

# Potencias de matriz diagonal
D_cuadrada = np.linalg.matrix_power(D, 2)
D_cubica = np.linalg.matrix_power(D, 3)

print("\nD²:")
print(D_cuadrada)
print("\nD³:")
print(D_cubica)
```

## Matriz Identidad

La **matriz identidad** $\mathbf{I}$ es una matriz diagonal con todos los elementos diagonales iguales a 1:

$$\mathbf{I}_n = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}$$

**Propiedad fundamental:**
$$\mathbf{I}_m\mathbf{A} = \mathbf{A}\mathbf{I}_n = \mathbf{A}$$

```{python}
# Matriz identidad
I3 = np.eye(3)
A = np.array([[2, 3, 1], [4, 1, 5], [2, 0, 3]])

IA = np.dot(I3, A)
AI = np.dot(A, I3)

print("Matriz Identidad I₃:")
print(I3)
print("\n¿I·A = A?", np.allclose(IA, A))
print("¿A·I = A?", np.allclose(AI, A))
```

## Matriz Nula

La **matriz nula** $\mathbf{0}$ tiene todos sus elementos iguales a cero:

$$\mathbf{0}_{m \times n} = \begin{bmatrix}
0 & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0
\end{bmatrix}$$

**Propiedades:**
- $\mathbf{A} + \mathbf{0} = \mathbf{A}$
- $\mathbf{A} \cdot \mathbf{0} = \mathbf{0}$

# Transpuesta de una Matriz

## Definición

La **transpuesta** $\mathbf{A}'$ (o $\mathbf{A}^T$) se obtiene intercambiando filas por columnas:

$$\text{Si } \mathbf{A} = [a_{ij}]_{m \times n} \implies \mathbf{A}' = [a_{ji}]_{n \times m}$$

```{python}
# Transpuesta
A = np.array([[1, 2, 3], [4, 5, 6]])
A_T = A.T

print("Matriz A (2×3):")
print(A)
print("\nTranspuesta A' (3×2):")
print(A_T)
```

## Propiedades de la Transpuesta

::: {.callout-tip}
## Propiedades Fundamentales

1. $(\mathbf{A}')' = \mathbf{A}$
2. $(\mathbf{A} + \mathbf{B})' = \mathbf{A}' + \mathbf{B}'$
3. $(\alpha\mathbf{A})' = \alpha\mathbf{A}'$
4. $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$ (orden inverso)
5. $(\mathbf{A}^{-1})' = (\mathbf{A}')^{-1}$
:::

```{python}
# Verificación de (AB)' = B'A'
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

AB = np.dot(A, B)
AB_T = AB.T
B_T_A_T = np.dot(B.T, A.T)

print("¿(AB)' = B'A'?", np.allclose(AB_T, B_T_A_T))
```

## Matrices Simétricas y Antisimétricas

### Matriz Simétrica

Una matriz cuadrada es **simétrica** si $\mathbf{A} = \mathbf{A}'$

$$a_{ij} = a_{ji} \quad \forall i, j$$

```{python}
# Matriz simétrica
A_sim = np.array([[5, 2, 1], [2, 3, 4], [1, 4, 6]])

print("Matriz simétrica A:")
print(A_sim)
print(f"\n¿Es simétrica? {np.allclose(A_sim, A_sim.T)}")
```

### Matriz Antisimétrica

Una matriz cuadrada es **antisimétrica** si $\mathbf{A} = -\mathbf{A}'$

$$a_{ij} = -a_{ji} \quad \forall i, j$$

```{python}
# Matriz antisimétrica
A_antisim = np.array([[0, 2, -3], [-2, 0, 5], [3, -5, 0]])

print("Matriz antisimétrica A:")
print(A_antisim)
print(f"\n¿Es antisimétrica? {np.allclose(A_antisim, -A_antisim.T)}")
```

# Determinantes

## Definición y Propiedades

El **determinante** es una función escalar que asigna a cada matriz cuadrada $\mathbf{A}$ un número real, denotado $|\mathbf{A}|$ o $\det(\mathbf{A})$.

### Determinante 2×2

$$|\mathbf{A}| = \begin{vmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{vmatrix}
= a_{11}a_{22} - a_{12}a_{21}$$

```{python}
# Determinante 2×2
A = np.array([[3, 1], [2, 4]])
det_A = np.linalg.det(A)

print("Matriz A:")
print(A)
print(f"\ndet(A) = {A[0,0]}·{A[1,1]} - {A[0,1]}·{A[1,0]}")
print(f"det(A) = {det_A:.4f}")
```

### Determinante 3×3 (Regla de Sarrus)

Para matrices 3×3, se puede usar la regla de Sarrus o la expansión de Laplace.

```{python}
# Determinante 3×3
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
det_A = np.linalg.det(A)

print("Matriz A:")
print(A)
print(f"\ndet(A) = {det_A:.10f}")
```

## Expansión de Laplace por Cofactores

La **Expansión de Laplace** permite calcular el determinante mediante **menores** y **cofactores**.

### Menores y Cofactores

::: {.callout-tip}
## Definiciones

**Menor** $|M_{ij}|$: Determinante de la submatriz obtenida al eliminar la fila $i$ y columna $j$.

**Cofactor** $|C_{ij}|$: Menor con signo algebraico:
$|C_{ij}| = (-1)^{i+j}|M_{ij}|$
:::

### Fórmula de Expansión

El determinante se puede expandir por cualquier fila $i$ o columna $j$:

$|\mathbf{A}| = \sum_{j=1}^{n} a_{ij}|C_{ij}| \quad \text{(por fila } i\text{)}$

$|\mathbf{A}| = \sum_{i=1}^{n} a_{ij}|C_{ij}| \quad \text{(por columna } j\text{)}$

```{python}
# Expansión por cofactores
from sympy import Matrix, symbols

A = Matrix([[2, 3, 1], [1, 4, 2], [3, 1, 5]])

print("Matriz A:")
display(A)
print(f"\nDeterminante |A| = {A.det()}")

# Cofactor de a₁₁
M_11 = A.minor(0, 0)
C_11 = (-1)**(0+0) * M_11
print(f"\nCofactor C₁₁ = (-1)^(1+1) × M₁₁ = {C_11}")
```

## Propiedades Fundamentales del Determinante

::: {.callout-tip}
## Propiedades Importantes

1. $|\mathbf{I}| = 1$
2. $|\mathbf{A}'| = |\mathbf{A}|$
3. Si dos filas (o columnas) son iguales, $|\mathbf{A}| = 0$
4. $|\mathbf{AB}| = |\mathbf{A}||\mathbf{B}|$
5. $|\alpha\mathbf{A}| = \alpha^n|\mathbf{A}|$ (para matriz $n \times n$)
6. $|\mathbf{A}^{-1}| = \frac{1}{|\mathbf{A}|}$ si $\mathbf{A}$ es no singular
:::

```{python}
# Verificación de propiedades
A = np.array([[2, 3], [1, 4]])
B = np.array([[5, 1], [2, 3]])

det_A = np.linalg.det(A)
det_B = np.linalg.det(B)
det_AB = np.linalg.det(np.dot(A, B))

print(f"det(A) = {det_A:.4f}")
print(f"det(B) = {det_B:.4f}")
print(f"det(AB) = {det_AB:.4f}")
print(f"det(A)·det(B) = {det_A * det_B:.4f}")
print(f"\n¿|AB| = |A||B|? {np.isclose(det_AB, det_A * det_B)}")
```

# No Singularidad y Rango de Matrices

## Condición de No Singularidad

Una matriz cuadrada $\mathbf{A}$ de orden $n \times n$ es **no singular** si cumple:

::: {.callout-important}
## Condiciones Equivalentes

1. **Determinante no nulo**: $|\mathbf{A}| \neq 0$
2. **Rango completo**: $r(\mathbf{A}) = n$
3. **Inversa existe**: $\mathbf{A}^{-1}$ está definida
4. **Filas/columnas linealmente independientes**
:::

Si $|\mathbf{A}| = 0$, la matriz es **singular** y no tiene inversa.

```{python}
# Matriz no singular vs singular
A_no_sing = np.array([[4, 2], [3, 1]])
A_sing = np.array([[2, 4], [1, 2]])

print("Matriz no singular:")
print(A_no_sing)
print(f"det(A) = {np.linalg.det(A_no_sing):.4f}")

print("\nMatriz singular:")
print(A_sing)
print(f"det(A) = {np.linalg.det(A_sing):.10f}")
```

## Rango de una Matriz

El **rango** $r(\mathbf{A})$ es el número máximo de filas (o columnas) linealmente independientes.

**Propiedades:**
- $r(\mathbf{A}) = r(\mathbf{A}')$
- $r(\mathbf{A}) \leq \min(m, n)$ para matriz $m \times n$
- $r(\mathbf{A}) = n$ para matriz cuadrada $n \times n$ no singular

```{python}
# Rango de matrices
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
rango_A = np.linalg.matrix_rank(A)

print("Matriz A:")
print(A)
print(f"\nRango r(A) = {rango_A}")
print(f"Dimensión: 3×3")
print(f"¿Rango completo? {rango_A == 3}")
```

# Matriz Inversa

## Definición y Existencia

La **inversa** $\mathbf{A}^{-1}$ de una matriz cuadrada $\mathbf{A}$ satisface:

$\mathbf{AA}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$

::: {.callout-important}
## Condición de Existencia

$\mathbf{A}^{-1}$ existe si y solo si:
- $\mathbf{A}$ es cuadrada
- $|\mathbf{A}| \neq 0$ (no singular)
:::

## Fórmula Mediante Matriz Adjunta

La **matriz adjunta** $\mathbf{A}^{\#}$ (o $\text{adj}\mathbf{A}$) es la transpuesta de la matriz de cofactores.

**Fórmula de la inversa:**
$\mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|} \mathbf{A}^{\#}$

```{python}
# Cálculo de la inversa
A = np.array([[4, 2], [3, 1]])
det_A = np.linalg.det(A)

if det_A != 0:
    A_inv = np.linalg.inv(A)
    
    print("Matriz A:")
    print(A)
    print(f"\nDeterminante: {det_A:.4f}")
    print("\nInversa A⁻¹:")
    print(A_inv)
    
    # Verificación
    print("\nVerificación A·A⁻¹:")
    print(np.dot(A, A_inv))
```

## Fórmula para Matrices 2×2

Para matrices 2×2, existe una fórmula explícita:

$\mathbf{A}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}$

donde $\mathbf{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$

```{python}
# Fórmula explícita para 2×2
A = np.array([[4, 2], [3, 1]])
a, b = A[0, 0], A[0, 1]
c, d = A[1, 0], A[1, 1]

det_A = a*d - b*c
A_inv_manual = (1/det_A) * np.array([[d, -b], [-c, a]])
A_inv_numpy = np.linalg.inv(A)

print("Inversa (fórmula manual):")
print(A_inv_manual)
print("\nInversa (NumPy):")
print(A_inv_numpy)
print(f"\n¿Son iguales? {np.allclose(A_inv_manual, A_inv_numpy)}")
```

## Propiedades de la Inversa

::: {.callout-tip}
## Propiedades Fundamentales

1. $(\mathbf{A}^{-1})^{-1} = \mathbf{A}$
2. $(\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}$ (orden inverso)
3. $(\mathbf{A}')^{-1} = (\mathbf{A}^{-1})'$
4. $|\mathbf{A}^{-1}| = \frac{1}{|\mathbf{A}|}$
:::

# Solución de Sistemas de Ecuaciones Lineales

## Método de Inversión de Matrices

Para el sistema $\mathbf{Ax = d}$, si $\mathbf{A}$ es no singular:

$\mathbf{x}^* = \mathbf{A}^{-1}\mathbf{d}$

```{python}
# Solución por inversión
A = np.array([[2, 1], [1, 3]])
d = np.array([[5], [8]])

A_inv = np.linalg.inv(A)
x = np.dot(A_inv, d)

print("Sistema Ax = d:")
print("\nMatriz A:")
print(A)
print("\nVector d:")
print(d.T)
print("\nSolución x = A⁻¹d:")
print(x.T)

# Verificación
print("\nVerificación Ax:")
print(np.dot(A, x).T)
```

## Regla de Cramer

La **Regla de Cramer** proporciona una fórmula explícita para cada variable:

$x_j^* = \frac{|\mathbf{A}_j|}{|\mathbf{A}|}$

donde $|\mathbf{A}_j|$ es el determinante de $\mathbf{A}$ con su columna $j$ reemplazada por $\mathbf{d}$.

```{python}
# Regla de Cramer
A = np.array([[2, 1], [1, 3]])
d = np.array([5, 8])

det_A = np.linalg.det(A)

# Para x₁: reemplazar primera columna
A1 = A.copy()
A1[:, 0] = d
det_A1 = np.linalg.det(A1)
x1 = det_A1 / det_A

# Para x₂: reemplazar segunda columna
A2 = A.copy()
A2[:, 1] = d
det_A2 = np.linalg.det(A2)
x2 = det_A2 / det_A

print(f"x₁ = |A₁|/|A| = {det_A1:.0f}/{det_A:.0f} = {x1:.0f}")
print(f"x₂ = |A₂|/|A| = {det_A2:.0f}/{det_A:.0f} = {x2:.0f}")
```

# Valores y Vectores Propios

## Definición

Para una matriz cuadrada $\mathbf{A}$, encontrar escalares $\lambda$ (**valores propios**) y vectores no nulos $\mathbf{v}$ (**vectores propios**) tales que:

$\mathbf{Av} = \lambda\mathbf{v}$

## Ecuación Característica

Reescribiendo: $(\mathbf{A} - \lambda\mathbf{I})\mathbf{v} = \mathbf{0}$

Para soluciones no triviales:
$|\mathbf{A} - \lambda\mathbf{I}| = 0$

Esta es la **ecuación característica** que determina los valores propios.

```{python}
# Valores y vectores propios
A = np.array([[4, 1], [2, 3]])

valores_propios, vectores_propios = np.linalg.eig(A)

print("Matriz A:")
print(A)
print("\nValores propios λ:")
for i, val in enumerate(valores_propios, 1):
    print(f"λ{i} = {val:.4f}")

print("\nVectores propios v:")
print(vectores_propios)
```

## Verificación y Propiedades

```{python}
# Verificación: Av = λv
v1 = vectores_propios[:, 0].reshape(-1, 1)
lambda1 = valores_propios[0]

Av1 = np.dot(A, v1)
lambda1_v1 = lambda1 * v1

print(f"Verificación para λ₁ = {lambda1:.4f}:")
print(f"\nAv₁ =")
print(Av1)
print(f"\nλ₁v₁ =")
print(lambda1_v1)
print(f"\n¿Son iguales? {np.allclose(Av1, lambda1_v1)}")
```

::: {.callout-tip}
## Propiedades

1. **Traza**: $\text{tr}(\mathbf{A}) = \sum_{i=1}^{n} \lambda_i$
2. **Determinante**: $|\mathbf{A}| = \prod_{i=1}^{n} \lambda_i$
3. **Matriz simétrica**: Todos los valores propios son reales
4. **Matriz positiva definida**: Todos los valores propios son positivos
:::

```{python}
# Verificación de propiedades
A = np.array([[5, 2], [2, 3]])
valores_propios = np.linalg.eigvals(A)

traza = np.trace(A)
suma_vp = np.sum(valores_propios)

determinante = np.linalg.det(A)
producto_vp = np.prod(valores_propios)

print(f"Traza: {traza}")
print(f"Suma valores propios: {suma_vp:.4f}")
print(f"\nDeterminante: {determinante:.4f}")
print(f"Producto valores propios: {producto_vp:.4f}")
```

# Aplicaciones Económicas

## Equilibrio de Mercado: Un Bien

Consideremos un mercado simple con demanda y oferta lineales:

- Demanda: $q_d = a - \beta p$
- Oferta: $q_s = -\gamma + \delta p$

Condición de equilibrio: $q_d = q_s$

```{python}
# Equilibrio de mercado simple
# Parámetros
a, beta = 50, 2
gamma, delta = 10, 3

# En equilibrio: a - β*p = -γ + δ*p
# (β + δ)*p = a + γ
p_eq = (a + gamma) / (beta + delta)
q_eq = a - beta * p_eq

print("Modelo de Equilibrio de Mercado")
print(f"\nDemanda: qd = {a} - {beta}p")
print(f"Oferta: qs = -{gamma} + {delta}p")
print(f"\nPrecio de equilibrio: p* = {p_eq:.2f}")
print(f"Cantidad de equilibrio: q* = {q_eq:.2f}")
```

## Equilibrio General: Dos Bienes

Sistema de ecuaciones para dos mercados relacionados:

```{python}
# Equilibrio general de 2 bienes
# Funciones de demanda y oferta
# Mercado 1: qd1 = 53 - 2p1 - 3p2, qs1 = -4 + 3p1
# Mercado 2: qd2 = 64 - 3p1 - 4p2, qs2 = -8 + 2p2

# Sistema en forma Ax = d
A = np.array([[5, 3], [3, 6]])
d = np.array([57, 72])

# Solución
p_eq = np.linalg.solve(A, d)
p1, p2 = p_eq

# Cantidades
q1 = -4 + 3*p1
q2 = -8 + 2*p2

print("Equilibrio General de 2 Bienes")
print(f"\nPrecios de equilibrio:")
print(f"p₁* = {p1:.2f}")
print(f"p₂* = {p2:.2f}")
print(f"\nCantidades de equilibrio:")
print(f"q₁* = {q1:.2f}")
print(f"q₂* = {q2:.2f}")

# Relación entre bienes
print("\nRelación entre bienes:")
print("Coeficiente de p₂ en qd₁: -3 (negativo)")
print("Coeficiente de p₁ en qd₂: -3 (negativo)")
print("→ Los bienes son COMPLEMENTOS")
```

## Equilibrio General: Tres Bienes

```{python}
# Equilibrio de 3 bienes
A = np.array([
    [2, -2, 1],
    [-1, 4, 2],
    [2, 1, 7]
])
d = np.array([9, 32, 63])

# Solución
precios = np.linalg.solve(A, d)
p1, p2, p3 = precios

# Cantidades
q1 = -6 + p1
q2 = -4 + 2*p2
q3 = -9 + 3*p3

print("Equilibrio General de 3 Bienes")
print(f"\nPrecios de equilibrio:")
print(f"p₁* = {p1:.2f}")
print(f"p₂* = {p2:.2f}")
print(f"p₃* = {p3:.2f}")
print(f"\nCantidades de equilibrio:")
print(f"q₁* = {q1:.2f}")
print(f"q₂* = {q2:.2f}")
print(f"q₃* = {q3:.2f}")
```

## Modelo de Ingreso Nacional (Keynesiano)

El modelo simple de ingreso nacional relaciona:
- $Y$: Renta Nacional
- $C$: Consumo
- $I_0$: Inversión (exógena)
- $G_0$: Gasto Público (exógeno)

**Ecuaciones:**
1. Identidad contable: $Y = C + I_0 + G_0$
2. Función de consumo: $C = \alpha + \beta Y$

```{python}
# Modelo de Ingreso Nacional
# Parámetros
alpha = 100  # Consumo autónomo
beta = 0.75  # Propensión marginal a consumir
I0 = 200     # Inversión
G0 = 150     # Gasto público

# Sistema: Y - C = I0 + G0
#         -βY + C = α
A = np.array([[1, -1], [-beta, 1]])
d = np.array([I0 + G0, alpha])

# Solución
solucion = np.linalg.solve(A, d)
Y_eq, C_eq = solucion

# Multiplicador
multiplicador = 1 / (1 - beta)

print("Modelo de Ingreso Nacional")
print(f"\nParámetros:")
print(f"Consumo autónomo α = {alpha}")
print(f"Propensión marginal a consumir β = {beta}")
print(f"Inversión I₀ = {I0}")
print(f"Gasto público G₀ = {G0}")
print(f"\nEquilibrio:")
print(f"Renta nacional Y* = {Y_eq:.2f}")
print(f"Consumo C* = {C_eq:.2f}")
print(f"\nMultiplicador = 1/(1-β) = {multiplicador:.4f}")
```

## Modelo Insumo-Producto de Leontief

El modelo de Leontief analiza las interrelaciones entre industrias.

**Ecuación fundamental:**
$\mathbf{x} = \mathbf{Ax} + \mathbf{d}$
$\mathbf{x} = (\mathbf{I} - \mathbf{A})^{-1}\mathbf{d}$

donde:
- $\mathbf{x}$: Vector de producción total
- $\mathbf{A}$: Matriz de coeficientes técnicos
- $\mathbf{d}$: Vector de demanda final

```{python}
# Modelo Insumo-Producto
# Matriz de coeficientes técnicos
A = np.array([
    [0.3, 0.2, 0.1],
    [0.2, 0.4, 0.2],
    [0.1, 0.1, 0.3]
])

# Demanda final
d = np.array([100, 150, 200])

# Matriz de tecnología
I = np.eye(3)
T = I - A

# Producción total requerida
x = np.linalg.solve(T, d)

print("Modelo Insumo-Producto de Leontief")
print("\nMatriz de coeficientes técnicos A:")
print(A)
print("\nDemanda final d:")
print(d)
print("\nMatriz de tecnología (I - A):")
print(T)
print("\nProducción total requerida x:")
for i, xi in enumerate(x, 1):
    print(f"Sector {i}: {xi:.2f}")

# Verificación: x = Ax + d
verificacion = np.dot(A, x) + d
print("\nVerificación Ax + d:")
print(verificacion)
print(f"¿Ax + d = x? {np.allclose(verificacion, x)}")
```

## Visualización del Modelo Insumo-Producto

```{python}
#| label: fig-insumo-producto
#| fig-cap: "Flujo de insumos entre sectores en el modelo de Leontief"

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Gráfica 1: Matriz de coeficientes
im1 = ax1.imshow(A, cmap='YlOrRd', aspect='auto')
ax1.set_xticks([0, 1, 2])
ax1.set_yticks([0, 1, 2])
ax1.set_xticklabels(['Sector 1', 'Sector 2', 'Sector 3'])
ax1.set_yticklabels(['Sector 1', 'Sector 2', 'Sector 3'])
ax1.set_title('Matriz de Coeficientes Técnicos', fontsize=14, fontweight='bold')

for i in range(3):
    for j in range(3):
        text = ax1.text(j, i, f'{A[i, j]:.1f}',
                       ha="center", va="center", color="black", fontsize=12)

plt.colorbar(im1, ax=ax1, label='Coeficiente')

# Gráfica 2: Comparación demanda vs producción
sectores = ['Sector 1', 'Sector 2', 'Sector 3']
x_pos = np.arange(len(sectores))
width = 0.35

ax2.bar(x_pos - width/2, d, width, label='Demanda Final', color='steelblue')
ax2.bar(x_pos + width/2, x, width, label='Producción Total', color='coral')

ax2.set_xlabel('Sectores', fontsize=12)
ax2.set_ylabel('Unidades', fontsize=12)
ax2.set_title('Demanda Final vs Producción Total Requerida', 
              fontsize=14, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels(sectores)
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

# Ejercicios Resueltos

## Ejercicio 1: Multiplicación de Matrices

**Problema:** Dados $\mathbf{A} = \begin{bmatrix} 2 & 3 \\ 1 & 4 \end{bmatrix}$ y $\mathbf{B} = \begin{bmatrix} 5 & 1 \\ 2 & 3 \end{bmatrix}$

Calcular: (a) $\mathbf{AB}$, (b) $\mathbf{BA}$, (c) $(\mathbf{AB})'$

```{python}
# Solución Ejercicio 1
A = np.array([[2, 3], [1, 4]])
B = np.array([[5, 1], [2, 3]])

AB = np.dot(A, B)
BA = np.dot(B, A)
AB_T = AB.T

print("(a) AB =")
print(AB)
print("\n(b) BA =")
print(BA)
print("\n(c) (AB)' =")
print(AB_T)
print(f"\n¿AB = BA? {np.array_equal(AB, BA)}")
```

## Ejercicio 2: Determinante e Inversa

**Problema:** Para $\mathbf{A} = \begin{bmatrix} 3 & 1 \\ 5 & 2 \end{bmatrix}$

Calcular: (a) $|\mathbf{A}|$, (b) $\mathbf{A}^{-1}$, (c) Verificar $\mathbf{AA}^{-1} = \mathbf{I}$

```{python}
# Solución Ejercicio 2
A = np.array([[3, 1], [5, 2]])

# (a) Determinante
det_A = np.linalg.det(A)
print(f"(a) |A| = {det_A:.0f}")

# (b) Inversa
A_inv = np.linalg.inv(A)
print("\n(b) A⁻¹ =")
print(A_inv)

# (c) Verificación
verificacion = np.dot(A, A_inv)
print("\n(c) AA⁻¹ =")
print(verificacion)
print(f"\n¿AA⁻¹ = I? {np.allclose(verificacion, np.eye(2))}")
```

## Ejercicio 3: Sistema de Ecuaciones (Regla de Cramer)

**Problema:** Resolver el sistema:
$\begin{cases}
2p_1 + p_2 = 5 \\
p_1 + 3p_2 = 8
\end{cases}$

```{python}
# Solución Ejercicio 3
A = np.array([[2, 1], [1, 3]])
d = np.array([5, 8])

# Método 1: Inversión de matrices
p = np.linalg.solve(A, d)
print("Solución por inversión:")
print(f"p₁ = {p[0]:.0f}")
print(f"p₂ = {p[1]:.0f}")

# Método 2: Regla de Cramer
det_A = np.linalg.det(A)

A1 = A.copy()
A1[:, 0] = d
p1 = np.linalg.det(A1) / det_A

A2 = A.copy()
A2[:, 1] = d
p2 = np.linalg.det(A2) / det_A

print("\nSolución por Regla de Cramer:")
print(f"p₁ = {p1:.0f}")
print(f"p₂ = {p2:.0f}")
```

## Ejercicio 4: Valores Propios

**Problema:** Encontrar valores y vectores propios de $\mathbf{A} = \begin{bmatrix} 6 & -2 \\ -2 & 3 \end{bmatrix}$

